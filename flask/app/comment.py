import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')


MODEL_NAME = "beomi/KcELECTRA-base"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

class CurseDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item["labels"] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)
    

model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)
model.to(device)
model.load_state_dict(torch.load('app/comment_test_model_state_dict.pt', map_location='cpu'))

# 0: curse, 1: non_curse
def sentence_predict(sent):
    # í‰ê°€ëª¨ë“œë¡œ ë³€ê²½
    model.eval()

    # ì…ë ¥ëœ ë¬¸ì¥ í† í¬ë‚˜ì´ì§•
    tokenized_sent = tokenizer(
        sent,
        return_tensors="pt",
        truncation=True,
        add_special_tokens=True,
        max_length=128
    )
    
    # ëª¨ë¸ì´ ìœ„ì¹˜í•œ GPUë¡œ ì´ë™ 
    tokenized_sent.to(device)

    # ì˜ˆì¸¡
    with torch.no_grad():
        outputs = model(
            input_ids=tokenized_sent["input_ids"],
            attention_mask=tokenized_sent["attention_mask"],
            token_type_ids=tokenized_sent["token_type_ids"]
            )

    # ê²°ê³¼ return
    logits = outputs[0]
    logits = logits.detach().cpu()
    result = logits.argmax(-1)
    if result == 0:
        result = " >> ì•…ì„±ëŒ“ê¸€ ğŸ‘¿"
    elif result == 1:
        result = " >> ì •ìƒëŒ“ê¸€ ğŸ˜€"
    return result


print(sentence_predict("ì•„ ì§„ì§œ ê·¸ëƒ¥ ì£½ì–´ë¼"))